#!/usr/bin/env python3 

import json
from typing import Dict, List, Any, Optional, TypedDict, Generator, Tuple, final
from gensim.models.doc2vec import Doc2Vec
from gensim.utils import tokenize
from gensim.parsing.preprocessing import *

from Comparisons.experiments.general import \
    load_yaml_cfg, load_contributor_commit_repo, load_contributor_index, load_repository_index, \
    dict_invert, \
    RepoDict

import numpy as np 
import re
import nltk
import os
import pickle
from tqdm import tqdm

cfg = load_yaml_cfg()['alpha']


class ContributorRepoEmbedding: 

    model:           Doc2Vec
    contributor_idx: Dict[str, int]           
    commitment:      Dict[int, Dict[int, int]]
    repo_idx_rvt:    Dict[int, str]           
    repos:           Dict[str, RepoDict]

    def __init__(self): 
        model = Doc2Vec.load(cfg['model']['dev2vec_repo_file_path'])   # output_vec_size: 230
        self.model = model
        with open(cfg['raw']['user_bio_file_path']) as fp: 
            self.bio_map = json.load(fp)

        self.commitment      = load_contributor_commit_repo()
        self.contributor_idx = load_contributor_index()
        self.repo_idx_rvt    = dict_invert(load_repository_index())

        with open(cfg['raw']['repo_file_path']) as fp:
            repos = (json.loads(it) for it in fp)
            self.repos = {it['name']: it for it in repos}


    def convert(self, user_name: str) -> np.ndarray: 

        # step 1: prepare user bio
        user_bio: str = self.bio_map.get(user_name)
        if user_bio is None: user_bio = 'none'

        # step 2: prepare repo names
        commitment = self.commitment
        idx2repo   = self.repo_idx_rvt
        user2idx   = self.contributor_idx

        user_idx = user2idx[user_name]
        repo_indices = commitment.get(user_idx) 
        if repo_indices is None: 
            print(f'Warning: developer {user_name} ({user_idx}) has no repo contributed.')
            repo_indices = []
            pass  
        repos: List[str] = [idx2repo[it] for it in repo_indices]

        # step 3. convert repo_names to RepoDict
        # step 4. collect the token generated by the user (|>tokens)
        tokens: List[List[str]] = []

        for repo_name in repos: 
            repo: RepoDict = self.repos.get(repo_name)
            if repo is None: continue

            # get repo['readme']
            readme_filepath = os.path.join(cfg['collect_data']['readme_directory'], repo_name.replace('/', '#'))
            if not os.path.exists(readme_filepath): 
                repo['readme'] = ''
                # print(f'readme not found for repo [{repo_name}]')
            else: 
                with open(os.path.join(cfg['collect_data']['readme_directory'])) as fp:
                    repo['readme'] = fp.read()

            tokens.append(self._tokenize_repo(
                user_bio, 
                repo['tags'], 
                repo['topic'] or 'none', 
                repo['readme'],
            ))

        # step 5. invoke the model, get the vector 
        return np.array(self.model\
                .infer_vector([it for l in tokens for it in l]))
    

    def _tokenize_repo(self,
            bio: str,         
            repo_tags: List[str],
            repo_topic: str, 
            repo_readme: str
    ) -> List[str]: 
        '''
            Details doesn't matter. 
            All you need to know is: 

            This function act as the compared paper does:
            given   a user's    bio, 
                and a repos's   repo_tags, repo_topic, repo_readme,
            It returns a list of tokens. 

            Mostly transcriptted from their_repo -> Dev2Vec_data_load_clean.ipynb. 
        '''

        words = set(nltk.corpus.words.words())  #

        def newline(text):
            return '\n'.join([p for p in re.split(r'\\n|\\r|\\n\\n|\\r\\n|\r\n', text) if len(p) > 0])

        repo_tags   = str(repo_tags)
        repo_topic  = newline(repo_topic)
        repo_readme = newline(repo_readme)

        text = '\n'.join([bio, repo_tags, repo_topic, repo_readme])
        gen = text.split()
        
        gen = (it.lower() for doc in gen for it in tokenize(doc, lower=True))  # token stream

        tokens: List[str] = preprocess_string(
            ' '.join(list(gen)), 
            filters=[remove_stopwords, split_alphanum, strip_numeric, strip_tags, strip_non_alphanum])
        tokens = (it for it in tokens if it.lower() in words or not it.isalpha())
        tokens = (it for it in tokens if len(it) > 2)
        return list(tokens)


def main():
    klee = ContributorRepoEmbedding()

    devlopers = load_contributor_index().keys()

    with open(cfg['embedding']['contributor_repo_embedding'], 'rb') as fp: 
        result = pickle.load(fp)
    try: 
        for dev_name in tqdm(devlopers): 
            if dev_name in result: continue
            result[dev_name] = klee.convert(dev_name)
    finally:
        with open(cfg['embedding']['contributor_repo_embedding'], 'wb') as fp: 
            pickle.dump(result, fp)