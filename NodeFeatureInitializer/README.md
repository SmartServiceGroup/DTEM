
> "你愿意了解这个世界底层的真相吗?" 

这个目录下呈现的是将原始数据转换成嵌入的过程. 

就我目前的观察, 这些步骤并没有希望保持嵌入向量和原数据之间的对应关系, 
只是将数据转换成可以用来训练模型的样子. 

> 按理说不是这样的, 因为后面训练GNN的时候会需要用到这样的对应关系. 应该是我哪里看漏了. 

简单说一下数据的流向. 

0. 数据来源: CHCrawler/cleaned/* 

1. 第一步: 数据清洗. 发生在如 ./PREmbedding/load_{code,text}_data.py 文件中. 这个文件将会读取上一步的数据, 按照论文中的说法做一定的转换. 转换后的数据存放到如 pr_descriptions.txt文件下. 这其实是一个jsonl文件. 

2. 第二步: 转换成嵌入向量. 看此目录下的脚本, 按前缀可以简单分为: `aggregate` 和 `embed` 两种. 不满足此条件的是 `embedder_{code,text}.py`, 是为其他脚本提供基础的: 顾名思义, 他们是为代码和文本提供嵌入向量的. `embed` 开头的脚本(如 `embed_pr_text.py`) 将读取上一步的输出, 以16个为一组, 将他们的嵌入向量输出为 jsonl 格式, 输出文件如 `pr_description_embedding.txt`.

3. 第三步: 合并相关的向量. 在 `aggregate` 打头的脚本中执行. 他们会读取上一步的 embedding.txt 文件, 并输出成 export/*.pkl 文件.


--------------------------------------------------------------------------------

存在的问题: 

1. 文件大小不对啊! 
   我按照最后一步生成(之前丢失的) `export/repo_code_embedding.pkl`. 
   因为脚本会同时生成 `export/repo_code_path_embedding.pkl`, 所以提前备份了. 
   但新生成的和原来的大小并不一致: 原来的是 21G, 新生成的只有 20G. 

   (但我不想追究了. 妈的. 反正最后论文也没我的名字, 名义上的"我的科研成果"罢了, 没撂挑子不干就算好的了)

2. 生成代码嵌入那部分代码没有细看. 

3. export下面还有其他文件, 如 lang2idx.json, lang2max 等. 这些应该是在处理离散数据的时候生成的. 之后可以仔细了解下它们是怎么来的. 

4. `parser` 目录看上去是 tree-sitter 的相关文件. 不知道会不会有用呢? 

